---
title: "Machine Learning Project"
author: "Allan R Brewer Cappellin"
date: "August 17, 2015"
output:
  pdf_document:
    latex_engine: xelatex
    fig_width: 6
    fig_height: 4
subtitle: Predict the quality of the exercise using machine learning in R.
fontsize: 10pt
geometry: margin = 1in
---

## Executive Summary

In this project we will train a machine learning model to determine if barbell lifts are done correctly. We must choose the varaibles and algorithim to use. The results must be validated against a testing data submited to the Coursera Project Submission system.

---

## Exploratory Data Analysis and Summary

```{r Load_Packages, echo=FALSE, message=FALSE, warning=FALSE}
library(caret)
library(rpart)
library(randomForest)
library(AppliedPredictiveModeling)
```

Firts we load the data sets using a vector for the different values that should be considered NA and discaded in our analysis.

```{r Load_Data, echo=TRUE}
# Load Train data from csv
dataset <- read.csv(file = "pml-training.csv", stringsAsFactors = FALSE, na.strings = c("NA", "NaN", "", " ", "#DIV/0!"))
# Load Exam data from csv
exam <- read.csv(file = "pml-testing.csv", stringsAsFactors = FALSE, na.strings = c("NA", "NaN", "", " ", "#DIV/0!"))
```

With the training set loaded into the environment we proceed to check the data and determine the variables to incorporate into the analysis. Using simple line of code we subset the training data to exclude all the varaibles that contain more than 99% NA values. We must also check the other variables to determine if they are good features for the prediction. With a short investigation we can determine that the varibales with the row number, user names, timestamp and window are not usefull for training the algorithim, they are removed. To finish with cleaning up the data set we will transform the "classe" variable to a factor since it is the one we are tryin to predict.


```{r variable_analysis, echo=TRUE}
# Eliminate variables with 99% NAs
data.ss <- dataset[,colSums(is.na(dataset)) < (0.01* nrow(dataset))]
rm(dataset)
# Eliminate not usefull varaibles
data.ss <- data.ss[,-(1:7)]
# Convert Classe variable to Factor
data.ss$classe <- as.factor(data.ss$classe)
# Separate Test data
set.seed(4321)
inTrain <- createDataPartition(data.ss$classe, p=0.70, list=FALSE)
trainning <- data.ss[ inTrain,]
testing <- data.ss[-inTrain,]
```

After cleaning the data we end up with a data set that can be used to train the model, a part of the set will be hold out to be tested later and determine the out of sample error. In the next section we will train the model and apply K-Fold Cross Validation using the train_control function.

## Machine Learning Model Training


```{r model_training, echo=FALSE}
# Set Seed
set.seed(1234)
# Define training control
train_control <- trainControl(number = 10)
customGrid <- data.frame(mtry=c(10, 26))
# Train the model 
modelFit <- train(classe ~ . ,trControl = train_control , data=trainning, method = "rf", tuneGrid=customGrid, ntree=100)
```


### Final Model


```{r model_summary, echo=FALSE}
# Print Fited Model
modelFit
# Print Model Summary
print(modelFit$finalModel)
```


```{r out_sample_error, echo=FALSE, fig.align='center'}
# Make predictions
predictions <- predict(modelFit, testing[,1:52])
# Summarize results
confusionMatrix(predictions, testing$classe)
```

## Conclusions

